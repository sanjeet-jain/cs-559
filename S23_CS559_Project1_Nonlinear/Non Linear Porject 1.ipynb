{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[70 pts] Task 1: Implementation\n",
    "\n",
    "•\t[60 pts] Implement LDA, Perceptron, and Logistic regression using NumPy only. \n",
    "\n",
    "•\t[5 pts] Fit the dataset and report the accuracy. Use sklearn.metrics.accuracy_score for the model accuracy calculation. All three models should have an accuracy higher than 80% when the dataset is not trained. \n",
    "\n",
    "•\t[5 pts] Make visualization of models and verify if models are acceptable. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports for entire project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import numpy as np\n",
    "np.random.seed(12)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data \n",
    "df = pd.read_csv(\"project1_1.csv\",header=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre processing the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.871319</td>\n",
       "      <td>0.490718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.715472</td>\n",
       "      <td>-0.458668</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.462538</td>\n",
       "      <td>-0.386599</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.327699</td>\n",
       "      <td>-0.240278</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         X         Y  Class\n",
       "0           0  0.871319  0.490718    0.0\n",
       "1           1  0.715472 -0.458668    1.0\n",
       "2           2  1.462538 -0.386599    1.0\n",
       "3           3 -0.222521  0.974928    0.0\n",
       "4           4  0.327699 -0.240278    1.0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "X             0\n",
       "Y             0\n",
       "Class         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  100 non-null    int64  \n",
      " 1   X           100 non-null    float64\n",
      " 2   Y           100 non-null    float64\n",
      " 3   Class       100 non-null    float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 3.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the first column as its just indices\n",
    "df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "\n",
    "# converted the class column to int as its 1 and 0\n",
    "df['Class'] = (df['Class'].astype(int))\n",
    "df['Class'] = df['Class'].replace(0,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum() )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data split and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization using mean and standard deviation \n",
    "\n",
    "X = df.drop(columns = ['Class'],axis =1)\n",
    "y = df['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_test_split(*arrays,  train_size=None, test_size=None,random_state=None):\n",
    "    \"\"\"Split arrays or matrices into random train and test subsets.\n",
    "    \"\"\"\n",
    "    # Determine the number of input arrays\n",
    "    n = len(arrays)\n",
    "    \n",
    "    # Check that all arrays have the same length\n",
    "    lengths = [len(arr) for arr in arrays]\n",
    "    if len(set(lengths)) > 1:\n",
    "        raise ValueError(\"All input arrays must have the same length.\")\n",
    "        \n",
    "    # Determine the number of samples and the size of the training set\n",
    "    n_samples = lengths[0]\n",
    "    if test_size is not None:\n",
    "        train_size = n_samples - int(n_samples * test_size)\n",
    "    elif train_size is None:\n",
    "        train_size = int(n_samples * 0.8)\n",
    "    else :\n",
    "        train_size = int(n_samples*train_size)\n",
    "        \n",
    "    # Generate a random permutation of the sample indices\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    # Split the arrays into training and testing sets\n",
    "    arrays_train = [arr[indices[:train_size]] for arr in arrays]\n",
    "    arrays_test = [arr[indices[train_size:]] for arr in arrays]\n",
    "    \n",
    "    # Return the training and testing sets as separate arrays\n",
    "    return arrays_train[0], arrays_test[0], arrays_train[1], arrays_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.to_numpy()\n",
    "y=y.to_numpy().reshape(-1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X,y,random_state=12,train_size=0.8)\n",
    "\n",
    "X_mean = np.mean(X_train, axis=0)\n",
    "X_std = np.std(X_train, axis=0)\n",
    "\n",
    "\n",
    "def scaleData(data,mean,std):\n",
    "    data_scaled = (data - mean) / std\n",
    "    return data_scaled\n",
    "# in this data set the classification has only 2 classes 0 and 1 but doing the scaling just as good practice \n",
    "X_train_scaled = scaleData(X_train ,X_mean , X_std)\n",
    "X_test_scaled = scaleData(X_test ,X_mean , X_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "When $\\lambda = 0$, the model is a regular logistic regression and when $\\lambda > 0$, it essentially becomes a regularized logistic regression.\n",
    "\n",
    "( i had done this code for cs 583 assignment so reusing it )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use mini batch gradient descent for the logistic regression model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent (MBGD)\n",
    "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_I and the gradient of Q_I\n",
    "# Inputs:\n",
    "#     w: weights: d-by-b matrix\n",
    "#     xi: data: b-by-d matrix\n",
    "#     yi: label: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def mb_objective_gradient(w, xi, yi, lam):\n",
    "    yx = np.multiply(yi,xi)\n",
    "    yxw = np.dot(yx,w)\n",
    "    expTerm1 = np.exp(-yxw)\n",
    "    logTerm = np.log(1+expTerm1)\n",
    "    obj = np.mean(logTerm) + (lam/2)*np.sum(w*w)\n",
    "    expTerm2 = np.exp(yxw)\n",
    "    g = np.mean( np.divide(-yx,1+expTerm2),axis = 0).reshape(xi.shape[1],1)+lam*w\n",
    "    return obj,g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
    "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def mbgd(x, y, lam, learning_rate, w, max_epoch=100, batchsize=10):\n",
    "    \n",
    "    objvals = np.zeros(max_epoch)\n",
    "    n = x.shape[0]\n",
    "    # epoch loop \n",
    "    for i in range(max_epoch):\n",
    "        # shuffling \n",
    "        randomIndices = np.random.permutation(n)\n",
    "        x_random, y_random = x[randomIndices, :], y[randomIndices, :]\n",
    "        currentObjValue = 0\n",
    "        for j in range(0,n,batchsize):\n",
    "            xi,yi = x_random[j:j+batchsize,:],y_random[j:j+batchsize,:]\n",
    "            currObj_at_j, gt_at_j = mb_objective_gradient(w,xi,yi,lam)\n",
    "            w -= learning_rate*gt_at_j\n",
    "            currentObjValue+= currObj_at_j\n",
    "        \n",
    "        learning_rate *= 0.95\n",
    "        objvals[i]= currentObjValue/(n/batchsize)\n",
    "        # print(f'Iteration number: {i+1} Current Objective Value: {objvals[i]}')\n",
    "    return w, objvals\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm of LDA can be summarized as follows:\n",
    "\n",
    "Compute the mean vectors $m_i$ for each class $i$.\n",
    "\n",
    "Compute the within-class scatter matrix $S_W$ and between-class scatter matrix $S_B$:\n",
    "\n",
    "$C$ is the number of classes in the labeled dataset\n",
    "\n",
    "a. $S_W = \\sum_{i=1}^C \\sum_{x \\in D_i} (x - m_i)(x - m_i)^T$\n",
    "\n",
    "b. $S_B = \\sum_{i=1}^C n_i (m_i - m)(m_i - m)^T$, where $m$ is the overall mean and $n_i$ is the number of samples in class $i$.\n",
    "\n",
    "Solve the generalized eigenvalue problem $S_B w = \\lambda S_W w$ to obtain the discriminant vectors $w_1, w_2, ..., w_{C-1}$.\n",
    "\n",
    "Sort the discriminant vectors by their corresponding eigenvalues in decreasing order.\n",
    "\n",
    "Select the $k$ most significant discriminant vectors, where $k \\leq C-1$.\n",
    "\n",
    "Project the data onto the subspace spanned by the selected discriminant vectors.\n",
    "\n",
    "Train a classifier using the transformed data.\n",
    "\n",
    "Use the classifier to predict the class labels of new instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(X,y):\n",
    "    class_means = []\n",
    "    #compute means vectors withing each class c \n",
    "    for c in np.unique(y):\n",
    "        X_c = X[y == c]\n",
    "        class_means.append(np.mean(X_c, axis=0))\n",
    "    # Compute within-class scatter matrix\n",
    "    S_w = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for c, class_mean in zip(np.unique(y), class_means):\n",
    "        X_c = X[y == c]\n",
    "        diff = X_c - class_mean\n",
    "        S_w += np.dot(diff.T, diff)\n",
    "    \n",
    "    # Compute between-class scatter matrix\n",
    "    overall_mean = np.mean(X, axis=0)\n",
    "    S_b = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for c, class_mean in zip(np.unique(y), class_means):\n",
    "        n = X[y == c].shape[0]\n",
    "        diff = class_mean - overall_mean\n",
    "        S_b += n * np.dot(diff.reshape(-1, 1), diff.reshape(1, -1))\n",
    "    \n",
    "    # Compute eigenvectors and eigenvalues of S_w^(-1) S_b\n",
    "    eig_vals, eig_vecs = np.linalg.eig(np.dot(np.linalg.inv(S_w), S_b))\n",
    "    \n",
    "    # Select the eigenvector with the largest eigenvalue as the projection direction\n",
    "    w = eig_vecs[:, np.argmax(eig_vals)]\n",
    "    \n",
    "    # Transform the samples onto the new subspace\n",
    "    X_lda = np.dot(X, w)\n",
    "    \n",
    "    return w, X_lda\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "lam = 0\n",
    "learning_rate = 1\n",
    "X_train_scaled_with_bias = np.hstack([X_train_scaled, np.ones((X_train_scaled.shape[0], 1))])\n",
    "w = np.zeros((X_train_scaled_with_bias.shape[1],1))\n",
    "w_mbgd, objvals_mbgd = mbgd(X_train_scaled_with_bias, y_train, lam, learning_rate, w,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA \n",
    "w_lda, X_lda = LDA(X_train_scaled,y_train.ravel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction\n",
    "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     X: data: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    f = np.sign(np.dot(X,w))\n",
    "    return np.asarray(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score for Logistic regression training is :  0.8875\n",
      "Training classification error for Logistic regression training is  0.11250000000000004\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (20,2) and (3,1) not aligned: 2 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[155], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining classification error for Logistic regression training is \u001b[39m\u001b[39m'\u001b[39m , \u001b[39m1\u001b[39m\u001b[39m-\u001b[39maccuracy_score_mbgd_train)\n\u001b[0;32m      9\u001b[0m X_test_scaled_with_bias \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack([X_test_scaled, np\u001b[39m.\u001b[39mones((X_test_scaled\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))])\n\u001b[1;32m---> 11\u001b[0m f_mbgd_test \u001b[39m=\u001b[39m predict(w_mbgd, X_test_scaled)\n\u001b[0;32m     12\u001b[0m f_mbgd_test\u001b[39m=\u001b[39mf_mbgd_test\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[0;32m     13\u001b[0m accuracy_score_mbgd_test \u001b[39m=\u001b[39m accuracy_score(y_test,f_mbgd_test)\n",
      "Cell \u001b[1;32mIn[154], line 8\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(w, X)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(w, X):\n\u001b[1;32m----> 8\u001b[0m     f \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msign(np\u001b[39m.\u001b[39;49mdot(X,w))\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(f)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (20,2) and (3,1) not aligned: 2 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Logistic regression \n",
    "f_mbgd_train = predict(w_mbgd, X_train_scaled_with_bias)\n",
    "f_mbgd_train=f_mbgd_train.astype(int)\n",
    "accuracy_score_mbgd_train = accuracy_score(y_train,f_mbgd_train)\n",
    "print('accuracy score for Logistic regression training is : ',accuracy_score_mbgd_train)\n",
    "print('Training classification error for Logistic regression training is ' , 1-accuracy_score_mbgd_train)\n",
    "\n",
    "\n",
    "X_test_scaled_with_bias = np.hstack([X_test_scaled, np.ones((X_test_scaled.shape[0], 1))])\n",
    "\n",
    "f_mbgd_test = predict(w_mbgd, X_test_scaled_with_bias)\n",
    "f_mbgd_test=f_mbgd_test.astype(int)\n",
    "accuracy_score_mbgd_test = accuracy_score(y_test,f_mbgd_test)\n",
    "print('accuracy score for Logistic regression test data is : ',accuracy_score_mbgd_test)\n",
    "print('Training classification error for Logistic regression test data is ' , 1-accuracy_score_mbgd_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "f_lda_train = predict(w_lda, X_train_scaled)\n",
    "f_lda_train=f_lda_train.astype(int)\n",
    "accuracy_score_lda_train = accuracy_score(y_train,f_lda_train)\n",
    "print('accuracy score for lda training is : ',accuracy_score_lda_train)\n",
    "print('Training classification error for lda training is ' , 1-accuracy_score_lda_train)\n",
    "\n",
    "f_lda_test = predict(w_lda, X_test_scaled)\n",
    "f_lda_test=f_lda_test.astype(int)\n",
    "accuracy_score_lda_test = accuracy_score(y_test,f_lda_test)\n",
    "print('accuracy score for lda  test data is : ',accuracy_score_lda_test)\n",
    "print('Training classification error for lda test data is ' , 1-accuracy_score_lda_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descaling data for graph plots\n",
    "\n",
    "f_mbgd_train=f_mbgd_train.astype(int)\n",
    "\n",
    "f_lda_train=f_lda_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1],  c=f_mbgd_train, cmap='viridis')\n",
    "plt.title('logistic regression training classification plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=f_lda_train, cmap='viridis')\n",
    "plt.title('lda training classification plot')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X_lda, np.zeros(X_lda.shape), c=y_train, cmap='viridis')\n",
    "plt.xlabel('LDA Component 1')\n",
    "plt.title('LDA Visualization')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
