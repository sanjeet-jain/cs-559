{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions and their derivatives\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    x = np.clip(x, -100, 100)  \n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    s = softmax(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Loss functions and their derivatives\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    return -np.log(y_pred[y_true==1]).mean()\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    return y_pred-y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        return sigmoid\n",
    "    if activation == \"relu\":\n",
    "        return relu\n",
    "    if activation == \"tanh\":\n",
    "        return tanh\n",
    "    if activation == \"softmax\":\n",
    "        return softmax\n",
    "\n",
    "def get_activation_derivative(activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        return sigmoid_derivative\n",
    "    if activation == \"relu\":\n",
    "        return relu_derivative\n",
    "    if activation == \"tanh\":\n",
    "        return tanh_derivative\n",
    "    if activation == \"softmax\":\n",
    "        return softmax_derivative\n",
    "\n",
    "def get_loss(loss):\n",
    "    if loss == \"mse\":\n",
    "        return mse\n",
    "    if loss==\"cross_entropy\":\n",
    "        return cross_entropy\n",
    "\n",
    "def get_loss_derivative(loss):\n",
    "    if loss == \"mse\":\n",
    "        return mse_derivative\n",
    "    if loss==\"cross_entropy\":\n",
    "        return cross_entropy_derivative\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # He normal initialization\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2 / input_size)\n",
    "        # self.weights = np.random.rand(output_size, input_size) - 0.5\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "        # self.bias = np.random.rand(output_size, 1) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.weights, self.input) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dL/dW, dL/dB for a given output_error=dL/dY. Returns input_error=dL/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        weights_error = np.outer(output_error, self.input)  # dL/dW = dL/dY * dY/dW\n",
    "        input_error = np.dot(self.weights.T, output_error)  # dL/dX = dL/dY * dY/dX\n",
    "        bias_error = output_error                       # dL/dB = dL/dY * dY/dB\n",
    "\n",
    "        # update parameters\n",
    "\n",
    "        self.weights -= learning_rate *weights_error\n",
    "        self.bias -= learning_rate*bias_error\n",
    "        return input_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_derivative):\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_derivative(self.input) * output_error\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "class Network:\n",
    "    def __init__(self, epochs=1000, learning_rate=0.001, loss_type=\"cross_entropy\", activation_type=\"relu\",hidden_layer_sizes=(100,10), tol =0.01, validation_fraction=0,patience=10):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = get_loss(loss_type)\n",
    "        self.loss_derivative = get_loss_derivative(loss_type)\n",
    "        self.activation = get_activation(activation_type)\n",
    "        self.activation_derivative = get_activation_derivative(activation_type)\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.layers = []\n",
    "        self.tol = tol\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.patience = patience  # number of epochs to wait if validation error doesn't improve\n",
    "        \n",
    "    def use(self, loss, loss_derivative):\n",
    "        self.loss = loss\n",
    "        self.loss_derivative = loss_derivative\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "   \n",
    "    def fit(self, X ,y):\n",
    "        X_train,y_train = X,y\n",
    "        X_valid = None\n",
    "        y_valid = None\n",
    "        if(self.validation_fraction>0):\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=self.validation_fraction)\n",
    "        \n",
    "        \n",
    "        self.layers = []\n",
    "        self.layers.append(FCLayer(X_train.shape[1], self.hidden_layer_sizes[0]))\n",
    "        self.layers.append(ActivationLayer(self.activation, self.activation_derivative))\n",
    "        #add hidden layers in a loop\n",
    "        for i in range(1, len(self.hidden_layer_sizes)):\n",
    "            self.layers.append(FCLayer(self.hidden_layer_sizes[i-1], self.hidden_layer_sizes[i]))\n",
    "            self.layers.append(ActivationLayer(self.activation, self.activation_derivative))\n",
    "\n",
    "        #output is always softmax \n",
    "        self.layers.append(FCLayer(self.hidden_layer_sizes[-1], 10))\n",
    "        self.layers.append(ActivationLayer(softmax,  softmax_derivative))\n",
    "        \n",
    "         # sample dimension first\n",
    "        samples = len(X_train)\n",
    "\n",
    "        # training loop\n",
    "        best_val_err = float('inf')  # initialize to large value\n",
    "         \n",
    "        wait = 0\n",
    "        for i in range(self.epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = X_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_derivative(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, self.learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, self.epochs, err))\n",
    "            \n",
    "            # val_err = 0\n",
    "            # if(len(X_valid)!=0 and len(y_valid)!=0):\n",
    "            #     # compute validation error\n",
    "            #     for j in range(len(X_valid)):\n",
    "            #         output = X_valid[j]\n",
    "            #         for layer in self.layers:\n",
    "            #             output = layer.forward_propagation(output)\n",
    "            #         val_err += self.loss(y_valid[j], output)\n",
    "            #     val_err /= samples\n",
    "            #     self.valErr=val_err\n",
    "            #     print('Validation error: %f' % val_err)\n",
    "\n",
    "            #     # early stopping\n",
    "            #     if val_err < best_val_err -self.tol:\n",
    "            #         best_val_err = val_err\n",
    "            #         best_weights = self.get_weights()\n",
    "            #         wait = 0  # reset wait counter\n",
    "            #     else:\n",
    "            #         wait += 1\n",
    "            #         if wait >= self.patience:\n",
    "            #             print('Early stopping: validation error has not improved significantly for %d epochs' % self.patience)\n",
    "            #             # restore best weights\n",
    "            #             self.set_weights(best_weights)\n",
    "            #             break\n",
    "            if X_valid is not None and y_valid is not None:\n",
    "                val_err = 0\n",
    "                for j in range(len(X_valid)):\n",
    "                    output = X_valid[j]\n",
    "                    for layer in self.layers:\n",
    "                        output = layer.forward_propagation(output)\n",
    "                    val_err += self.loss(y_valid[j], output)\n",
    "                val_err /= len(X_valid)\n",
    "                self.valErr = val_err\n",
    "                print('Validation error: %f' % val_err)\n",
    "\n",
    "                if val_err < best_val_err - self.tol:\n",
    "                    best_val_err = val_err\n",
    "                    best_weights = self.get_weights()\n",
    "                    wait = 0\n",
    "                else:\n",
    "                    wait += 1\n",
    "                    if wait >= self.patience:\n",
    "                        print('Early stopping: validation error has not improved significantly for %d epochs' % self.patience)\n",
    "                        break\n",
    "        if X_valid is not None and y_valid is not None:\n",
    "            self.set_weights(best_weights)  \n",
    "\n",
    "    def get_weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, FCLayer):\n",
    "                weights.append(layer.weights)\n",
    "                weights.append(layer.bias)\n",
    "        return weights\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        index = 0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, FCLayer):\n",
    "                layer.weights = weights[index]\n",
    "                layer.bias = weights[index+1]\n",
    "                index += 2\n",
    "                            \n",
    "      # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i].reshape(input_data.shape[1], -1)\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "    \n",
    "    def score(self, x_test, y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        y_pred = np.argmax(y_pred , axis=1)\n",
    "        y_test = np.argmax(y_test , axis=1)\n",
    "        return np.sum(y_pred == y_test) / len(y_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([5, 0, 4, ..., 8, 4, 8], dtype=int64))\n",
      "Valid set: (array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([3, 8, 6, ..., 5, 6, 8], dtype=int64))\n",
      "Test set: (array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([7, 2, 1, ..., 4, 5, 6], dtype=int64))\n",
      "X_train shape: (50000, 784)\n",
      "y_train shape: (50000,)\n",
      "X_valid shape: (10000, 784)\n",
      "y_valid shape: (10000,)\n",
      "X_test shape: (10000, 784)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "train_set, valid_set, test_set = None , None, None \n",
    "with gzip.open('./mnist-1.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set  = pickle.load(f, encoding='latin1')\n",
    "\n",
    "# Check that the datasets are loaded correctly\n",
    "print(\"Train set:\", train_set)\n",
    "print(\"Valid set:\", valid_set)\n",
    "print(\"Test set:\", test_set)\n",
    "\n",
    "# Access the train_set variable\n",
    "X_train, y_train = train_set\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "X_valid, y_valid = valid_set\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"y_valid shape:\", y_valid.shape)\n",
    "\n",
    "X_test, y_test = test_set\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "# Preprocess validation and test data sets\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 784)\n",
      "y_train shape: (60000,)\n",
      "X_train shape: (60000, 784, 1)\n",
      "y_train shape: (60000, 10, 1)\n",
      "X_test shape: (10000, 784, 1)\n",
      "y_test shape: (10000, 10, 1)\n",
      "X_valid shape: (10000, 784, 1)\n",
      "y_valid shape: (10000, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "# this is under the assumption that the validation data which we loaded earlier is not there in the training data and needs to be appended to the end \n",
    "# we append validation set to the end of the training set so validation fraction can take it as validation data when we do grid search\n",
    "X_train= np.concatenate((X_train,X_valid))\n",
    "y_train= np.concatenate((y_train,y_valid))\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np.eye(10)[y_train].reshape(y_train.shape[0], 10, 1)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "y_test = np.eye(10)[y_test].reshape(y_test.shape[0], 10, 1)\n",
    "\n",
    "#same for validation data \n",
    "X_valid = X_valid.reshape(X_valid.shape[0], X_valid.shape[1], 1)\n",
    "y_valid = np.eye(10)[y_valid].reshape(y_valid.shape[0], 10, 1)\n",
    "\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"y_valid shape:\", y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Network(epochs=10,learning_rate=0.01,loss_type=\"cross_entropy\",activation_type=\"relu\",hidden_layer_sizes=(100,50),validation_fraction=X_train.shape[0]/(X_valid.shape[0]*100), tol = 0.00001, patience=1000)\n",
    "# # net.add(FCLayer(784,100))\n",
    "# # net.add(ActivationLayer(relu, relu_derivative))\n",
    "# # net.add(FCLayer(100, 10))\n",
    "# # net.add(ActivationLayer(softmax, softmax_derivative))\n",
    "\n",
    "# # train\n",
    "# net.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Accuracy for training samples: {net.score(X_train, y_train)}')\n",
    "# print(f'Accuracy for validation samples: {net.score(X_valid, y_valid)}')\n",
    "# print(f'Accuracy for test samples: {net.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # create a dictionary with hyperparameters to search over\n",
    "# param_grid = {\n",
    "#     'epochs': [500, 1000, 2000],\n",
    "#     'learning_rate': [0.001, 0.01, 0.1],\n",
    "#     'hidden_layer_sizes': [(100,), (100, 10), (100, 10, 5)],\n",
    "#     'activation_type': ['relu', 'sigmoid', 'tanh', 'softmax'],\n",
    "#     'loss_type':['mse','cross_entropy']\n",
    "# }\n",
    "\n",
    "# # create an instance of your model\n",
    "# def create_network(epochs=1000, learning_rate=0.001, loss_type=\"cross_entropy\", activation_type=\"relu\",\n",
    "#                     hidden_layer_sizes=(100,10), tol=0.001, validation_fraction=0):\n",
    "#     return Network(epochs=epochs, learning_rate=learning_rate, loss_type=loss_type, activation_type=activation_type,\n",
    "#                    hidden_layer_sizes=hidden_layer_sizes, tol=tol, validation_fraction=validation_fraction)\n",
    "\n",
    "# # model = Network(validation_fraction=X_train.shape[0]/(X_valid.shape[0]*100))\n",
    "\n",
    "# # create a GridSearchCV object\n",
    "# grid_search = GridSearchCV(create_network(validation_fraction=X_train.shape[0]/(X_valid.shape[0]*100)), param_grid,n_jobs=-1)\n",
    "\n",
    "# # fit the GridSearchCV object to your data\n",
    "# #again not doing it over the complete dataset\n",
    "# grid_search.fit(X_train[:1000],y_train[:1000])\n",
    "\n",
    "# # get the best hyperparameters and model\n",
    "# best_params = grid_search.best_params_\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# print(\"Best hyperparameters:\", best_params)\n",
    "# print(\"Best estimator:\", best_model)\n",
    "# # evaluate the best model on your test data\n",
    "# accuracy = best_model.get_accuracy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'epochs': [10, 20, 50, 100],\n",
    "    'learning_rate': [0.0001,0.001, 0.01, 0.1],\n",
    "    'hidden_layer_sizes': [(10,), (100, 10), (100,50), (100,50,10)],\n",
    "    'activation_type': ['relu', 'sigmoid', 'tanh', 'softmax'],\n",
    "    'loss_type':['mse','cross_entropy'],\n",
    "    'validation_fraction':[0,X_train.shape[0]/(X_valid.shape[0]*100)]\n",
    "}\n",
    "\n",
    "# Define a custom function to train and evaluate the model\n",
    "def train_and_evaluate_model(params):\n",
    "    # Instantiate the model with the given params\n",
    "    model = Network(patience=100,**params)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    scoreValid = model.score(X_valid, y_valid)\n",
    "    scoreTest = model.score(X_test, y_test)\n",
    "\n",
    "    # Return the score along with the params\n",
    "    return scoreValid,scoreTest, params\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "param_list = list(ParameterGrid(param_grid))\n",
    "\n",
    "# Evaluate each combination of parameters\n",
    "results = []\n",
    "for params in param_list:\n",
    "    scoreValid,scoreTest, params = train_and_evaluate_model(params)\n",
    "    results.append((scoreValid,scoreTest, params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the results by score in descending order\n",
    "results.sort(key=lambda x: (x[0]),reverse=True)\n",
    "print(results)\n",
    "\n",
    "# Print the results\n",
    "for scoreValid,scoreTest, params in results:\n",
    "    print(\"scoreValid: {:.4f}\".format(scoreValid))\n",
    "    print(\"scoreTest: {:.4f}\".format(scoreTest))\n",
    "    print(\"Params: \", params)\n",
    "    print()\n",
    "\n",
    "# Get the best params and score\n",
    "best_scoreValid,best_scoreTest, best_params = results[0]\n",
    "print(\"best_scoreValid: {:.4f}\".format(best_scoreValid))\n",
    "print(\"best_scoreTest: {:.4f}\".format(best_scoreTest))\n",
    "print(\"Best params: \", best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_venv",
   "language": "python",
   "name": "tf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
